
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous">

  <style type="text/css">
  /* Design Credits: Deepak Pathak, Jon Barron and Abhishek Kar and Saurabh Gupta*/
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 400
  }
  heading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 19px;
    font-weight: 1000
  }
  strong {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 800
  }
  strongred {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    color: 'red' ;
    font-size: 16px
  }
  sectionheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    font-weight: 600
  }
  </style>
  <link rel="icon" type="image/png" href="images/seal_icon.png">
  <script type="text/javascript" src="js/hidebib.js"></script>
  <title>Rafael Valle</title>
  <meta name="Rafael Valle's Homepage" http-equiv="Content-Type" content="Rafael Valle's Homepage">
  <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
  <!-- Start : Google Analytics Code -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-99756592-1', 'auto');
    ga('send', 'pageview');
  </script>
  <!-- End : Google Analytics Code -->
  <!-- Scramble Script by Jeff Donahue -->
  <script src="js/scramble.js"></script>
</head>

<body>
<table width="840" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr><td>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <p align="center"><font size="7">Rafael Valle</font><br>
    <b>Email</b>:
    <font id="email" style="display:inline;">
      <noscript><i>Please enable Javascript to view</i></noscript>
    </font>
    <script>
    emailScramble = new scrambledString(document.getElementById('email'),
        'emailScramble', 'lfbkae@araeeeyvlled.lure',
        [5, 2, 12, 15, 7, 13, 11, 3, 14, 1, 4, 10, 16, 19, 6, 8, 17, 21, 22, 20, 9, 23, 0, 18]);
    </script>
  </p>

  <tr>
    <td width="67%" valign="middle" align="justify">
    <p>I'm a polymath senior research scientist at <a target="_blank" href="http://www.nvidia.com/">NVIDIA</a>
    focusing on audio and computer vision applications. I am passionate about generative modeling, 
    machine perception and machine improvisation.<br>â€¨ 
    Normalizing Flows >>> VAEs >>> GANs <p>
    <p>During my PhD at <a target="_blank" href="http://www.berkeley.edu/">UC Berkeley</a> I was
    advised mainly by <a target="_blank"
        href="https://people.eecs.berkeley.edu/~sseshia/">Prof. Sanjit Seshia</a> and <a target="_blank"
        href="http://edmundcampion.com/">Prof. Edmund Campion</a> and my research
    focused on machine listening and improvisation. At <a target="_blank"
        href="http://www.berkeley.edu/">UC Berkeley</a>, I was part of the <a
        target="_blank" href="https://www.terraswarm.org/">TerraSwarm Research
        Center</a>, where I worked on problems related to <a target="_blank"
    href="https://blog.openai.com/adversarial-example-research/">adversarial
    attacks</a> and <a target="_blank" href="https://arxiv.org/abs/1606.08514">verified artificial intelligence.</a></p> 

    <p>During Fall 2016 I was a Research Intern at <a target="_blank"
        href="http://www.gracenote.com/">Gracenote</a> in Emeryville, where I
    worked on audio classification using Deep Learning. Previously I was a
    Scientist Intern at <a target="_blank"
    href="http://www.pandora.com">Pandora</a> in Oakland, where I investigated
segments and scores that describe novelty seeking behavior in listeners.
    
    <p>Before coming to Berkeley, I completed a master's in Computer Music from <a target="_blank" href="https://www.hmdk-stuttgart.de/en/home/">HMDK Stuttgart</a> in Germany and a bachelor's in Orchestral Conducting from <a target="_blank" href="http://www.ufrj.br">UFRJ</a> in Brazil.</p>

    </td>

    <td width="33%"><a target="_blank" href="images/rafael_valle.png"><img src="images/rafael_valle.png" width="90%"></a>
      <ul class="network-icon" aria-hidden="true" style="text-align: left; padding-left: 5;">
        <a href="valle_resume_info_en_nopic_smaller.pdf" target="_blank" rel="noopener">
          <i class="ai ai-cv ai-2x big-icon" style="padding-right:5"></i>
        </a>
        <a href="https://twitter.com/rafaelvalleart" target="_blank" rel="noopener">
          <i class="fab fa-twitter fa-2x big-icon" style="padding-right:5"></i>
        </a>
        <a href="https://scholar.google.com/citations?user=SktxU8IAAAAJ&hl" target="_blank" rel="noopener">
          <i class="ai ai-google-scholar ai-2x big-icon" style="padding-right:5"></i>
        </a>
        <a href="https://www.linkedin.com/in/vallerafael" target="_blank" rel="noopener">
          <i class="fab fa-linkedin fa-2x big-icon" style="padding-right:5"></i>
        </a>
        <a href="https://github.com/rafaelvalle" target="_blank" rel="noopener">
          <i class="fab fa-github fa-2x big-icon"></i>
        </a>
      </ul>
    </td>
  </tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr><td>
    <sectionheading>News</sectionheading>
    <ul>     
    <li> <a href="#KEYWORD">Paper</a> about improving keyword spotting with synthetic speech. </li>
    <li> <a href="#FLOWTRON">Paper</a> about text-to-spectrogram model with more realism and expressivity than current SOTA. </li>
    <li> <a href="#NEURALODE">Paper</a> about a novel approach for image segmentation combining Neural ODEs and the Level Set method. </li>
    </ul>
  </td></tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr><td><sectionheading>Publications</sectionheading></td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr>
    <td width="33%" valign="top" align="center">
      <img src="images/improving_kws.png" style="width: 200px"></img>

    <td width="67%" valign="top">
      <p>
      <img src="images/new.png" alt="[NEW]" width="6%" style="border-style: none">
      <heading>Improving Keyword Spotting with Synthetic Speech</heading></a><br>
      U.  Vaidya, <strong>Rafael Valle</strong>,  M. Jain, U. Ahmed, V. Karandikar, S. S. Chauhan, Bryan Catanzaro<br>
      <br></p>

      <div class="paper" id="keyword">
        <a href="javascript:toggleblock('keyword_abs')">abstract</a>    
	<p align="justify"> <i id="keyword_abs">In this paper we describe a method that uses text-to-speech (TTS) synthesis models to improve the quality of keyword spotting models and to reduce the time and money required to train them. We synthesize varied data from different speakers by combining Flowtron, a multispeaker text-to-mel-spectrogram synthesis model producing speech with high variance, and WaveGlow, a universal mel-spectrogram to audio model. We fine-tune the synthetic data by using QuartzNet, an automatic speech recognition model, to find and remove samples with skipped, repeated and mispronounced words. With this fine-tuned synthetic data and 10% of human data we are able to achieve keyword spotting scores (accuracy and F1) that are comparable to using the full human dataset. We provide results on binary and multiclass Wake-up-Word datasets, including the Speech Commands Dataset.</i></p>
      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center">
        <img src="https://nv-adlr.github.io/images/flowtron_logo.png" style="width: 200px"></img>
        <audio controls preload="none" style="width: 200px"><source src="http://docs.google.com/uc?export=open&id=1bngjG6bMUWQi8aIugWhrFPWNHrNAokZF" type="audio/mpeg">audio not supported</audio>

    <td width="67%" valign="top">
      <p><a target="_blank" href="https://arxiv.org/abs/2005.05957" id="FLOWTRON">
      <img src="images/new.png" alt="[NEW]" width="6%" style="border-style: none">
      <heading>Flowtron: an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis</heading></a><br>
      <strong>Rafael Valle</strong>, Kevin Shih, Ryan Prenger, Bryan Catanzaro<br>
      <em>arXiv</em> 2019<br>
      <br></p>

      <div class="paper" id="flowtron">
        <a target="_blank" href="https://arxiv.org/abs/2005.05957">pdf</a> | 
        <a target="_blank" href="https://nv-adlr.github.io/Flowtron">samples</a> |
        <a href="javascript:toggleblock('flowtron_abs')">abstract</a>    
	<p align="justify"> <i id="flowtron_abs">In our recent paper, we
	  propose Flowtron: an autoregressive flow-based generative network for
	  text-to-speech synthesis with control over speech variation and style
	  transfer. Flowtron combines insights from IAF and optimizes Tacotron 2
	  in order to provide high-quality and controllable mel-spectrogram synthesis.        
	</i></p>
      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center">
      <img src="images/ecssd_0282_contour.png" style="width: 200px"></img>
    <td width="67%" valign="top">
      <p><a target="_blank" href="https://arxiv.org/abs/1912.11683" id="NEURALODE">
      <img src="images/new.png" alt="[NEW]" width="6%" style="border-style: none">
      <heading>Neural ODEs for Image Segmentation with Level Sets</heading></a><br>
      <strong>Rafael Valle</strong>, Fitsum Reda, Mohammad Shoeybi, Patrick Legresley, Andrew Tao, Bryan Catanzaro<br>
      <em>arXiv</em> 2019<br>
      <br></p>

      <div class="paper" id="flowtron">
        <a target="_blank" href="https://arxiv.org/pdf/1912.11683.pdf">pdf</a> | 
        <a href="javascript:toggleblock('node_abs')">abstract</a>    
	<p align="justify"> <i id="node_abs">We propose a novel approach for image segmentation that combines Neural Ordinary Differential Equations (NODEs) and the Level Set method.  Our approach parametrizes the evolution of an initial contour with a NODE that implicitly learns from data a speed function describing the evolution.  In addition, for cases where an initial contour is not available and to alleviate the need for careful choice or design of contour embedding functions, we propose a NODE-based method that evolves an image embedding into a dense per-pixel semantic label space. We evaluate our methods on kidney segmentation (KiTS19) and on salient object detection (PASCAL-S, ECSSD and HKU-IS). In addition to improving initial contours provided by deep learning models while using a fraction of their number of parameters, our approach achieves F scores that are higher than several state-of-the-art deep learning algorithms
	</i></p>
      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center">
        <img src="https://nv-adlr.github.io/images/mellotron_logo.png" style="width: 200px"></img>
        <audio controls preload="none" style="width: 200px"><source src="http://docs.google.com/uc?export=open&id=1QFWDsrt9-iGY63bpKE-u-V1X5kE1wAvO" type="audio/mpeg">audio not supported</audio>

    <td width="67%" valign="top">
      <p><a target="_blank" href="https://arxiv.org/abs/1910.11997" id="MELLOTRON">
      <heading>Mellotron: Multispeaker expressive voice synthesis by conditioning on rhythm, pitch and global style tokens</heading></a><br>
      <strong>Rafael Valle*</strong>, Jason Li*, Ryan Prenger, Bryan Catanzaro<br>
      <em>arXiv</em> 2019 - <em>ICASSP</em> 2020<br>
      <br></p>

      <div class="paper" id="mellotron">
        <a target="_blank" href="https://arxiv.org/abs/1910.11997">pdf</a> | 
        <a target="_blank" href="https://nv-adlr.github.io/Mellotron">samples</a> |
        <a href="javascript:toggleblock('mellotron_abs')">abstract</a>    
        <p align="justify"> <i id="mellotron_abs">Mellotron is a multispeaker
          voice synthesis model based on Tacotron 2 GST that can make a
          voice emote and sing without emotive or singing training data. By
          explicitly conditioning on rhythm and continuous pitch contours
          from an audio signal or music score, Mellotron is able to generate
          speech in a variety of styles ranging from read speech to
          expressive speech, from slow drawls to rap and from monotonous
          voice to singing voice.  
        </i></p>
      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center">
        <img src="https://nv-adlr.github.io/images/waveglow_logo.png" style="width: 200px"></img>
        <audio controls preload="none" style="width: 200px"<source src="http://docs.google.com/uc?export=open&id=1KOHZIr7iTupo13EAKpbdmjcbATPsV02i" type="audio/mpeg">audio not supported</audio>

    <td width="67%" valign="top">
      <p><a target="_blank" href="https://arxiv.org/abs/1811.00002" id="WAVEGLOW">
      <heading>WaveGlow: a Flow-based Generative Network for Speech Synthesis</heading></a><br>
      Ryan Prenger, <strong>Rafael Valle</strong>, Bryan Catanzaro<br>
      <em>ICASSP</em> 2019<br>
      <br></p>

      <div class="paper" id="waveglow">
        <a target="_blank" href="https://arxiv.org/abs/1807.04919">pdf</a> | 
        <a target="_blank" href="https://nv-adlr.github.io/WaveGlow">samples</a> |
        <a href="javascript:toggleblock('waveglow_abs')">abstract</a>    
        <p align="justify"> <i id="waveglow_abs"> We propose WaveGlow: a
          flow-based network capable of generating high quality speech from
          mel-spectrograms. WaveGlow combines insights from Glow and WaveNet in
          order to provide fast, efficient and high-quality audio synthesis,
          without the need for auto-regression. WaveGlow is implemented using
          only a single network, trained using only a single cost function:
          maximizing the likelihood of the training data, which makes the
          training procedure simple and stable.
        </i></p>
      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a target="_blank" href="https://github.com/rafaelvalle/ipgans/"><img src="images/ipgan.png" alt="sym" width="100%" style="border-style: none"></a>
    <td width="67%" valign="top">
      <p><a target="_blank" href="https://github.com/rafaelvalle/ipgans/blob/master/latex/ipgans.pdf" id="IPGAN">
      <heading>TequilaGAN: How to easily identify GAN samples</heading></a><br>
      <strong>Rafael Valle</strong>, Wilson Cai and Anish Doshi<br>
      <em>arXiv</em> 2018<br>
      <br></p>

      <div class="paper" id="ipgan">
        <a target="_blank" href="https://arxiv.org/abs/1807.04919">pdf</a> | 
        <a href="javascript:toggleblock('ipgan_abs')">abstract</a>    
        <p align="justify"> <i id="ipgan_abs"> In this paper we show
          strategies to easily identify fake samples generated with the
          Generative Adversarial Network framework. One strategy is based on
          the statistical analysis and comparison of raw pixel values and
          features extracted from them.  The other strategy learns formal
          specifications from the real data and shows that fake samples
          violate the specifications of the real data.  We show that fake
          samples produced with GANs have a universal signature that can be
          used to identify fake samples. We provide results on MNIST,
          CIFAR10, music and speech data. 
        </i></p>
      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a target="_blank" href="https://github.com/rafaelvalle/asrgen"><img src="images/conf_mat_cnn_knn.png" alt="sym" width="49%" style="border-style: none">&nbsp;<img src="images/pred_comparisson_spk0.png" alt="sym" width="49%" style="border-style: none"></a>
    <td width="67%" valign="top">
      <p><a target="_blank" href="https://github.com/rafaelvalle/asrgen/blob/master/main.pdf" id="ASRGEN">
      <heading>Attacking Speaker Recognition with Deep Generative Models</heading></a><br>
      Anish Doshi, Wilson Cai and <strong>Rafael Valle</strong><br>
      <em>arXiv</em> 2017<br>

      <div class="paper" id="asrgen">
      <a target="_blank" href="https://arxiv.org/pdf/1801.02384.pdf">pdf</a> | 
      <a href="javascript:toggleblock('asrgen_abs')">abstract</a> |
      <a target="_blank" href="https://github.com/rafaelvalle/asrgen">code</a>
      <p align="justify"> <i id="asrgen_abs"> In this paper we investigate the
          ability of generative adversarial networks (GANs) to synthesize
          spoofing attacks on modern speaker recognition systems. We first show
          that samples generated with SampleRNN and WaveNet are unable to fool a
          CNN-based speaker recognition system. We propose a modification of the
          Wasserstein GAN objective function to make use of data that is real
          but not from the class being learned.  Our semi-supervised learning
          method is able to perform both targeted and untargeted attacks,
      raising questions related to security in speaker authentication systems.
      </i></p>
      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center">
        <iframe width="100%" height="300" scrolling="yes" frameborder="no"
                                                          src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/playlists/325582916&amp;color=0066cc&amp;auto_play=false&amp;hide_related=true&amp;show_comments=false&amp;show_user=false&amp;show_reposts=false&amp;show_artwork=false">
        </iframe>    
    <td width="67%" valign="top">
      <p><a target="_blank" href="http://github.com/rafaelvalle/neural_network_control_improvisation" id="SEQGAN">
      <heading>Sequence Generation with GANs</heading></a><br>
      <strong>Rafael Valle</strong><br>
      2017
      <br></p>

      <div class="paper" id="seqgan">
      <a target="_blank" href="http://github.com/rafaelvalle/neural_network_control_improvisation">github</a> |
      <a href="javascript:toggleblock('seqgan_abs')">abstract</a> |
      <a target="_blank" href="https://soundcloud.com/d_alma/sets/improved-wasserstein-gans-piano">audio</a>
      <p align="justify"> <i id="seqgan_abs">In this paper we investigate the generation of sequences using generative adversarial networks (GANs). We open the paper by providing a brief introduction to sequence generation and challenges in GANs. We briefly describe encoding strategies for text and MIDI data in light of their use with convolutional architectures. In our experiments we consider the unconditional generation of polyphonic and monophonic piano roll generation as well as short sequences. For each data type, we provide sonic or text examples of generated data, interpolation in the latent space and vector arithmetic.</i></p>
      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a target="_blank" href="https://arxiv.org/pdf/1607.07801.pdf"><img src="images/abroa.png" alt="sym" width="75%" style="border-style: none"></a>
    <td width="67%" valign="top">
      <p><a target="_blank" href="https://github.com/rafaelvalle/machine_listening" id="ABROA">
      <heading>Audio-Based Room Occupancy Analysis using Gaussian Mixtures and Hidden Markov Models</heading></a><br>
      <strong>Rafael Valle</strong><br>
      <em>Future Technologies Conference (FTC)</em> 2016<br>
      <em>Detection and Classification of Acoustic Scenes and Events </em> 2016
      <br></p>

      <div class="paper" id="abroa">
      <a target="_blank" href="https://arxiv.org/pdf/1607.07801.pdf">pdf</a> |
      <a href="javascript:toggleblock('abroa_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('abroa')" class="togglebib">bibtex</a> |
      <a target="_blank" href="https://arxiv.org/abs/1607.07801">arXiv</a> |
      <a target="_blank" href="https://github.com/rafaelvalle/machine_listening">code</a>

      <p align="justify"> <i id="abroa_abs">This paper outlines preliminary steps towards the development of an audio based room-occupancy analysis model. Our approach borrows from speech recognition tradition and is based on Gaussian Mixtures and Hidden Markov Models. We analyze possible challenges encountered in the development of such a model, and offer several solutions including feature design and prediction strategies. We provide results obtained from experiments with audio data from a retail store in Palo Alto, California. Model assessment is done via leave-two-out Bootstrap and model convergence achieves good accuracy, thus representing a contribution to multimodal people counting algorithms.</i></p>

      <pre xml:space="preserve">
      @article{valle2016abroa,
        title={ABROA: Audio-Based Room-Occupancy Analysis using Gaussian Mixtures and Hidden Markov Models},
        author={Valle, Rafael},
        journal={arXiv preprint arXiv:1607.07801},
        year={2016}
      }
      </pre>
      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a target="_blank" href="https://github.com/rafaelvalle/mdi"><img src="images/mdi.png" alt="sym" width="75%" style="border-style: none"></a>
    <td width="67%" valign="top">
      <p><a target="_blank" href="https://github.com/rafaelvalle/mdi" id="MDI">
      <heading>Missing Data Imputation for Supervised Classification</heading></a><br>
      Jason Poulos and <strong>Rafael Valle</strong><br>
      <em>Applied Artificial Intelligence</em> 2018
      <br></p>

      <div class="paper" id="mdi">
      <a target="_blank" href="https://arxiv.org/pdf/1610.09075.pdf">pdf</a> |
      <a href="javascript:toggleblock('mdi_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('mdi')" class="togglebib">bibtex</a> |
      <a target="_blank" href="https://arxiv.org/pdf/1610.09075">arXiv</a> | 
      <a target="_blank" href="https://github.com/rafaelvalle/mdi">code</a>

      <p align="justify"> <i id="mdi_abs">This paper compares methods for imputing missing categorical data for supervised learning tasks. The ability of researchers to accurately fit a model and yield unbiased estimates may be compromised by missing data, which are prevalent in survey-based social science research. We experiment on two machine learning benchmark datasets with missing categorical data, comparing classifiers trained on non-imputed (i.e., one-hot encoded) or imputed data with different degrees of missing data perturbation. The results show imputation methods can increase predictive accuracy in the presence of missing-data perturbation. Additionally, we find that for imputed models, missing data perturbation can improve prediction accuracy by regularizing the classifier.</i></p>

      <pre xml:space="preserve">
      @article{poulos2016missing,
        title={Missing Data Imputation for Supervised Learning},
        author={Poulos, Jason and Valle, Rafael},
        journal={arXiv preprint arXiv:1610.09075},
        year={2016}
      }
      </pre>
      </div>
    </td>
  </tr>  


  <tr>
    <td width="33%" valign="top" align="center"><a target="_blank" href="https://github.com/rafaelvalle/music_pattern_graphs"><img src="images/pattgraph.png" alt="sym" width="75%" style="border-style: none"></a>
    <td width="67%" valign="top">
      <p><a target="_blank"
          href="https://github.com/rafaelvalle/music_pattern_graphs" id="PATTGRAPH">
      <heading>Learning and Visualizing Music Specifications using Pattern Graphs</heading></a><br>
      <strong>Rafael Valle</strong>, Daniel Fremont, Ilge Akkaya, Alexandre Donze, Adrian Freed and Sanjit Seshia<br>
      <em>ISMIR</em> 2016
      <br></p>

      <div class="paper" id="pattgraph">
      <a target="_blank" href="https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/280_Paper.pdf">pdf</a> |
      <a href="javascript:toggleblock('pattgraph_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('pattgraph')" class="togglebib">bibtex</a> |
      <a target="_blank" href="https://github.com/rafaelvalle/music_pattern_graphs">code</a>

      <p align="justify"> <i id="pattgraph_abs">We describe a system to learn and visualize specifications from song(s) in symbolic and audio formats. The core of our approach is based on a software engineering procedure called specification mining. Our procedure extracts patterns from feature vectors and uses them to build pattern graphs. The feature vectors are created by segmenting song(s) and extracting time and and frequency domain features from them, such as chromagrams, chord degree and interval classification. The pattern graphs built on these feature vectors provide the likelihood of a pattern between nodes, as well as start and ending nodes. The pattern graphs learned from a song(s) describe formal specifications that can be used for human interpretable quantitatively and qualitatively song comparison or to perform supervisory control in machine improvisation. We offer results in song summarization, song and style validation and machine improvisation with formal specifications.</i></p>

      <pre xml:space="preserve">
      @inproceedings{valle2016learning,
        title={Learning and Visualizing Music Specifications using Pattern Graphs},
        author={Valle, Rafael and Fremont, Daniel J and Akkaya, Ilge and Donze, Alexandre and Freed, Adrian and Seshia, Sanjit S},
        booktitleaddon= {Proceedings of the Seventeenth ISMIR Conference}        
        booktitle={ISMIR},
        year={2016}
      }      
      </pre>
      </div>
    </td>
  </tr>  
</table>

<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('keyword_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('flowtron_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('node_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('mellotron_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('waveglow_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('asrgen_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('ipgan_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('abroa_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('seqgan_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('mdi_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('pattgraph_abs');
</script>
</body>

</html>
